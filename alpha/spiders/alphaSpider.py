# -*- coding: utf-8 -*-
import scrapy
from alpha.items import ArticleList
from scrapy.selector import Selector
from scrapy.http.request import Request
from scrapy import Spider
from alpha.items import Article
from datetime import datetime

class AlphaSpider(scrapy.Spider):
    """
    This class defines the rules used to extract inforamtion from the urls we scape.
    First, collect all of the urls of a page and then issue a request for each url.
    This request results in a page corresponding to a article from that list. Extract
    the relevant information from that article.
    """
    name = "article_list"
    allowed_domains = ["seekingalpha.com"]

    custom_settings = {
        'ARTICLE_LIST_PIPELINE_ENABLED': True
    }

    base_url = 'www.seekingalpha.com/'
    protocol = 'http'
    start_urls = ['http://seekingalpha.com/articles?page=2']

    # for i in range(2,5):
    #     next_link = base_url + str(i)
    #     start_urls.append(next_link)

    def parse(self, response):
        """
        For every url on the article list pages, extract the urls of the articles
        on those pages
        """
        sel = Selector(response)
        url_list = sel.xpath('//*[@id="content_wrapper"]/div/div[2]/div[2]/div[2]/ul/*/div/a/@href').extract()
        article_urls = []

        # Create article list full urls
        for url in url_list:
            article_urls.append(self.protocol + '://' + self.base_url + url)

        # Generate the follow-up urls to actually scrape
        for article_url in article_urls:
            yield Request(article_url, callback=self.parse_articles)

    def parse_articles(self, response):
        """
        Parse each of the article urls generated by AlphaSpider.parse
        """
        sel = Selector(response)
        item = Article()
        item['article_id'] = response.request.url.split('/')[5].split('-')[0]
        item['article_url'] = str(response.request.url)
        item['pub_date'] = self.get_pub_date(sel)
        item['title'] = self.get_title(sel)
        item['author'] = self.get_author(sel)
        item['author_url'] = self.get_author_url(sel)
        item['covered'] = self.get_covered(sel)
        print(item)
        print(item['article_id'])
        return item


    def get_pub_date(self, selector):
        raw_date = selector.xpath('//*[@id="a-hd"]/div[1]/time/@content').extract()
        date = datetime.strptime(raw_date[0][:10], '%Y-%m-%d').date()
        return date


    def get_title(self, selector):
        title = selector.xpath('//*[@id="a-hd"]/h1/text()').extract()
        return str(title[0])


    def get_author(self, selector):
        author = selector.xpath('//*[@id="author-hd"]/div[2]/div[1]/a/span/text()').extract()
        return str(author[0])


    def get_author_url(self, selector):
        author_url = selector.xpath('//*[@id="author-hd"]/div[2]/div[1]/a/@href').extract()
        return str(author_url[0])


    def get_covered(self, selector):
        covered = selector.xpath('//*[@id="about_primary_stocks"]/a/@href').extract()
        if covered == []:
            covered = selector.xpath('//*[@id="about_stocks"]/*/@href').extract()
        return covered







